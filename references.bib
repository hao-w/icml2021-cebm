@article{khemakhem2020ice,
  title={ICE-BeeM: Identifiable conditional energy-based deep models},
  author={Khemakhem, Ilyes and Monti, Ricardo Pio and Kingma, Diederik P and Hyv{\"a}rinen, Aapo},
  journal={arXiv preprint arXiv:2002.11537},
  year={2020}
}

@inproceedings{gutmann2010noise,
  title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages={297--304},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{hyvarinen2005estimation,
  title={Estimation of non-normalized statistical models by score matching.},
  author={Hyv{\"a}rinen, Aapo and Dayan, Peter},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={4},
  year={2005}
}

@inproceedings{bao_bi-level_2020,
	title = {Bi-level {Score} {Matching} for {Learning} {Energy}-based {Latent} {Variable} {Models}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/d25a34b9c2a87db380ecd7f7115882ec-Abstract.html},
	language = {en},
	urldate = {2021-01-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bao, Fan and Li, Chongxuan and Xu, Taufik and Su, Hang and Zhu, Jun and Zhang, Bo},
	year = {2020},
	file = {Snapshot:C\:\\Users\\babak\\Zotero\\storage\\M7MV7UG3\\d25a34b9c2a87db380ecd7f7115882ec-Abstract.html:text/html;Full Text PDF:C\:\\Users\\babak\\Zotero\\storage\\7LYP9IV8\\Bao et al. - 2020 - Bi-level Score Matching for Learning Energy-based .pdf:application/pdf},
}

@article{aneja2020ncp,
  title={NCP-VAE: Variational Autoencoders with Noise Contrastive Priors},
  author={Aneja, Jyoti and Schwing, Alexander and Kautz, Jan and Vahdat, Arash},
  journal={arXiv preprint arXiv:2010.02917},
  year={2020}
}


@article{pang2020learning,
  title={Learning Latent Space Energy-Based Prior Model},
  author={Pang, Bo and Han, Tian and Nijkamp, Erik and Zhu, Song-Chun and Wu, Ying Nian},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{gao2020flow,
  title={Flow contrastive estimation of energy-based models},
  author={Gao, Ruiqi and Nijkamp, Erik and Kingma, Diederik P and Xu, Zhen and Dai, Andrew M and Wu, Ying Nian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7518--7528},
  year={2020}
}

@incollection{hinton2012practical,
  title={A practical guide to training restricted Boltzmann machines},
  author={Hinton, Geoffrey E},
  booktitle={Neural networks: Tricks of the trade},
  pages={599--619},
  year={2012},
  publisher={Springer}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}

@article{nalisnick2018deep,
  title={Do deep generative models know what they don't know?},
  author={Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1810.09136},
  year={2018}
}

@article{hendrycks2016baseline,
  title={A baseline for detecting misclassified and out-of-distribution examples in neural networks},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1610.02136},
  year={2016}
}

@inproceedings{tieleman2008training,
  title={Training restricted Boltzmann machines using approximations to the likelihood gradient},
  author={Tieleman, Tijmen},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1064--1071},
  year={2008}
}

@inproceedings{johnson2016composing,
  title = {Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Johnson, Matthew and Duvenaud, David K. and Wiltschko, Alex and Adams, Ryan P. and Datta, Sandeep R.},
  year = {2016},
  pages = {2946--2954},
  file = {/Users/janwillem/Cloud/Zotero/storage/JK5GQLFV/Johnson - 2016 - Composing graphical models with neural networks for structured representations.pdf;/Users/janwillem/Cloud/Zotero/storage/NSBSNYDM/6378-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-in.html}
}

@inproceedings{esmaeili2019structured,
  title={Structured disentangled representations},
  author={Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, Narayanaswamy and Paige, Brooks and Brooks, Dana H and Dy, Jennifer and Meent, Jan-Willem},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2525--2534},
  year={2019},
  organization={PMLR}
}

@inproceedings{esmaeili2019structuredb,
  title = {Structured {{Neural Topic Models}} for {{Reviews}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Esmaeili, Babak and Huang, Hongyi and Wallace, Byron and van de Meent, Jan-Willem},
  year = {2019},
  month = apr,
  pages = {3429--3439},
  abstract = {We present Variational Aspect-based Latent Topic Allocation (VALTA), a family of autoencoding topic models that learn aspect-based representations of reviews. VALTA defines a user-item encoder that...},
  annotation = {00000},
  file = {/Users/janwillem/Cloud/Zotero/storage/GSMNVYWF/Esmaeili et al. - 2019 - Structured Neural Topic Models for Reviews.pdf},
  language = {en}
}

@inproceedings{wu2020amortized,
  title = {Amortized Population Gibbs Samplers with Neural Sufficient Statistics},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {Wu, Hao and Zimmermann, Heiko and Sennesh, Eli and Le, Tuan Anh and {van de Meent}, Jan-Willem},
  year = {2020},
  pages = {10205--10215},
  file = {/Users/janwillem/Cloud/Zotero/storage/IADV6C84/Wu - 2020 - Amortized Population Gibbs Samplers with Neural Sufficient Statistics.pdf}
}


@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}


@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}
@article{williams1992simple,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  volume = {8},
  issn = {0885-6125},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  number = {3-4},
  journal = {Machine Learning},
  doi = {10.1007/BF00992696},
  author = {Williams, Ronald J.},
  year = {1992},
  keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
  pages = {229-256},
  file = {/Users/janwillem/Zotero/storage/FP24PEJJ/Williams - 1992 - Simple statistical gradient-following algorithms for connectionist.pdf}
}

%%%%%%%%%%%%%%AIR-style models%%%%%%%%%%%%%%
@inproceedings{eslami2016attend,
  title={Attend, infer, repeat: Fast scene understanding with generative models},
  author={Eslami, SM Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and Hinton, Geoffrey E and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3225--3233},
  year={2016}
}

@inproceedings{kosiorek2018sequential,
  title={Sequential attend, infer, repeat: Generative modelling of moving objects},
  author={Kosiorek, Adam and Kim, Hyunjik and Teh, Yee Whye and Posner, Ingmar},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8606--8616},
  year={2018}
}

@article{law2019take,
  title = {Take a {{Look Around}}: {{Using Street View}} and {{Satellite Images}} to {{Estimate House Prices}}},
  shorttitle = {Take a {{Look Around}}},
  author = {Law, Stephen and Paige, Brooks and Russell, Chris},
  year = {2019},
  month = sep,
  volume = {10},
  pages = {54:1--54:19},
  issn = {2157-6904},
  doi = {10.1145/3342240},
  abstract = {When an individual purchases a home, they simultaneously purchase its structural features, its accessibility to work, and the neighborhood amenities. Some amenities, such as air quality, are measurable while others, such as the prestige or the visual impression of a neighborhood, are difficult to quantify. Despite the well-known impacts intangible housing features have on house prices, limited attention has been given to systematically quantifying these difficult to measure amenities. Two issues have led to this neglect. Not only do few quantitative methods exist that can measure the urban environment, but that the collection of such data is both costly and subjective. We show that street image and satellite image data can capture these urban qualities and improve the estimation of house prices. We propose a pipeline that uses a deep neural network model to automatically extract visual features from images to estimate house prices in London, UK. We make use of traditional housing features such as age, size, and accessibility as well as visual features from Google Street View images and Bing aerial images in estimating the house price model. We find encouraging results where learning to characterize the urban quality of a neighborhood improves house price prediction, even when generalizing to previously unseen London boroughs. We explore the use of non-linear vs. linear methods to fuse these cues with conventional models of house pricing, and show how the interpretability of linear models allows us to directly extract proxy variables for visual desirability of neighborhoods that are both of interest in their own right, and could be used as inputs to other econometric methods. This is particularly valuable as once the network has been trained with the training data, it can be applied elsewhere, allowing us to generate vivid dense maps of the visual appeal of London streets.},
  file = {/Users/janwillem/Cloud/Zotero/storage/U97V8Y2H/Law et al. - 2019 - Take a Look Around Using Street View and Satellit.pdf},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  keywords = {computer vision,convolutional neural network,deep learning,hedonic price models,London,Real estate},
  number = {5}
}

@inproceedings{crawford2019spatially,
  title={Spatially invariant unsupervised object detection with convolutional neural networks},
  author={Crawford, Eric and Pineau, Joelle},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3412--3420},
  year={2019}
}

@article{crawford2019exploiting,
  title={Exploiting Spatial Invariance for Scalable Unsupervised Object Tracking},
  author={Crawford, Eric and Pineau, Joelle},
  journal={arXiv preprint arXiv:1911.09033},
  year={2019}
}

@article{lin2020improving,
  title={Improving Generative Imagination in Object-Centric World Models},
  author={Lin, Zhixuan and Wu, Yi-Fu and Peri, Skand and Fu, Bofeng and Jiang, Jindong and Ahn, Sungjin},
  journal={arXiv preprint arXiv:2010.02054},
  year={2020}
}
@article{burgess2019monet,
  title={Monet: Unsupervised scene decomposition and representation},
  author={Burgess, Christopher P and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1901.11390},
  year={2019}
}

@article{greff2019multi,
  title={Multi-object representation learning with iterative variational inference},
  author={Greff, Klaus and Kaufmann, Rapha{\"e}l Lopez and Kabra, Rishab and Watters, Nick and Burgess, Chris and Zoran, Daniel and Matthey, Loic and Botvinick, Matthew and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1903.00450},
  year={2019}
}
%%%%%%%%%%%%%%Information Bottleneck%%%%%%%%%%%%%%
@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@article{alemi2017fixing,
  title={Fixing a broken ELBO},
  author={Alemi, Alexander A and Poole, Ben and Fischer, Ian and Dillon, Joshua V and Saurous, Rif A and Murphy, Kevin},
  journal={arXiv preprint arXiv:1711.00464},
  year={2017}
}


@article{engelcke2019genesis,
  title={Genesis: Generative scene inference and sampling with object-centric latent representations},
  author={Engelcke, Martin and Kosiorek, Adam R and Jones, Oiwi Parker and Posner, Ingmar},
  journal={arXiv preprint arXiv:1907.13052},
  year={2019}
}

%%%%%%%%%%%contrastive predictive coding#########
@article{kipf2019contrastive,
  title={Contrastive Learning of Structured World Models},
  author={Kipf, Thomas and van der Pol, Elise and Welling, Max},
  journal={arXiv preprint arXiv:1911.12247},
  year={2019}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{henaff2019data,
  title={Data-efficient image recognition with contrastive predictive coding},
  author={H{\'e}naff, Olivier J and Razavi, Ali and Doersch, Carl and Eslami, SM and Oord, Aaron van den},
  journal={arXiv preprint arXiv:1905.09272},
  year={2019}
}

@article{hjelm2018learning,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1808.06670},
  year={2018}
}





@incollection{siddharth2017learning,
  title = {Learning {{Disentangled Representations}} with {{Semi}}-{{Supervised Deep Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Siddharth, N. and Paige, T. Brooks and {van de Meent}, Jan-Willem and Desmaison, Alban and Goodman, Noah and Kohli, Pushmeet and Wood, Frank and Torr, Philip},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5927--5937},
  publisher = {{Curran Associates, Inc.}},
  copyright = {All rights reserved},
  file = {/Users/janwillem/Cloud/Zotero/storage/C7SM9SU6/Siddharth - 2017 - Learning Disentangled Representations with Semi-Supervised Deep Generative.pdf}
}


@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@inproceedings{kim2018disentangling,
  title={Disentangling by Factorising},
  author={Kim, Hyunjik and Mnih, Andriy},
  booktitle={International Conference on Machine Learning},
  pages={2649--2658},
  year={2018}
}

@article{lin2020space,
  title = {{{SPACE}}: {{Unsupervised Object}}-{{Oriented Scene Representation}} via {{Spatial Attention}} and {{Decomposition}}},
  shorttitle = {{{SPACE}}},
  author = {Lin, Zhixuan and Wu, Yi-Fu and Peri, Skand Vishwanath and Sun, Weihao and Singh, Gautam and Deng, Fei and Jiang, Jindong and Ahn, Sungjin},
  year = {2020},
  month = mar,
  abstract = {The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page},
  archivePrefix = {arXiv},
  eprint = {2001.02407},
  eprinttype = {arxiv},
  file = {/Users/janwillem/Cloud/Zotero/storage/SZDV42WR/Lin et al. - 2020 - SPACE Unsupervised Object-Oriented Scene Represen.pdf;/Users/janwillem/Cloud/Zotero/storage/GSZY4AU8/2001.html},
  journal = {arXiv:2001.02407 [cs, eess, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@inproceedings{eslami2016air,
  title = {Attend, Infer, Repeat: Fast Scene Understanding with Generative Models},
  shorttitle = {Attend, Infer, Repeat},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and Kavukcuoglu, Koray and Hinton, Geoffrey E.},
  year = {2016},
  month = dec,
  pages = {3233--3241},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene -without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.},
  file = {/Users/janwillem/Cloud/Zotero/storage/FE8QAK5R/Eslami et al. - 2016 - Attend, infer, repeat fast scene understanding wi.pdf},
  isbn = {978-1-5108-3881-9},
  series = {{{NIPS}}'16}
}

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}

@inproceedings{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={Advances in neural information processing systems},
  pages={6626--6637},
  year={2017}
}

@article{yu2020training,
  title={Training Deep Energy-Based Models with f-Divergence Minimization},
  author={Yu, Lantao and Song, Yang and Song, Jiaming and Ermon, Stefano},
  journal={arXiv preprint arXiv:2003.03463},
  year={2020}
}

@inproceedings{tomczak2018vae,
  title={VAE with a VampPrior},
  author={Tomczak, Jakub and Welling, Max},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1214--1223},
  year={2018}
}

@inproceedings{nijkamp2019learning,
  title={Learning non-convergent non-persistent short-run MCMC toward energy-based model},
  author={Nijkamp, Erik and Hill, Mitch and Zhu, Song-Chun and Wu, Ying Nian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5232--5242},
  year={2019}
}

@article{nijkamp2019anatomy,
  title={On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based Models},
  author={Nijkamp, Erik and Hill, Mitch and Han, Tian and Zhu, Song-Chun and Nian Wu, Ying},
  journal={arXiv},
  pages={arXiv--1903},
  year={2019}
}
@article{du2019implicit,
  title={Implicit generation and generalization in energy-based models},
  author={Du, Yilun and Mordatch, Igor},
  journal={arXiv preprint arXiv:1903.08689},
  year={2019}
}

@article{vincent2011connection,
  title={A connection between score matching and denoising autoencoders},
  author={Vincent, Pascal},
  journal={Neural computation},
  volume={23},
  number={7},
  pages={1661--1674},
  year={2011},
  publisher={MIT Press}
}

@article{liu2020hybrid,
  title={Hybrid discriminative-generative training via contrastive learning},
  author={Liu, Hao and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2007.09070},
  year={2020}
}

@inproceedings{xie2016theory,
  title={A theory of generative convnet},
  author={Xie, Jianwen and Lu, Yang and Zhu, Song-Chun and Wu, Yingnian},
  booktitle={International Conference on Machine Learning},
  pages={2635--2644},
  year={2016}
}

@inproceedings{abdal2019image2stylegan,
  title={Image2stylegan: How to embed images into the stylegan latent space?},
  author={Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4432--4441},
  year={2019}
}

@inproceedings{mukherjee2019clustergan,
  title={Clustergan: Latent space clustering in generative adversarial networks},
  author={Mukherjee, Sudipto and Asnani, Himanshu and Lin, Eugene and Kannan, Sreeram},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={4610--4617},
  year={2019}
}

@inproceedings{tran2017disentangled,
  title={Disentangled representation learning gan for pose-invariant face recognition},
  author={Tran, Luan and Yin, Xi and Liu, Xiaoming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1415--1424},
  year={2017}
}

@inproceedings{kosaraju2019social,
  title={Social-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention networks},
  author={Kosaraju, Vineet and Sadeghian, Amir and Mart{\'\i}n-Mart{\'\i}n, Roberto and Reid, Ian and Rezatofighi, Hamid and Savarese, Silvio},
  booktitle={Advances in Neural Information Processing Systems},
  pages={137--146},
  year={2019}
}

@inproceedings{salakhutdinov2009deep,
  title={Deep boltzmann machines},
  author={Salakhutdinov, Ruslan and Hinton, Geoffrey},
  booktitle={Artificial intelligence and statistics},
  pages={448--455},
  year={2009}
}

@inproceedings{grathwohl2021no,
title={No {\{}MCMC{\}} for me: Amortized sampling for fast and stable training of energy-based models},
author={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ixpSxO9flk3}
}

@article{du2020improved,
  title={Improved Contrastive Divergence Training of Energy Based Models},
  author={Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  journal={arXiv preprint arXiv:2012.01316},
  year={2020}
}

@inproceedings{song2020sliced,
  title={Sliced score matching: A scalable approach to density and score estimation},
  author={Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={574--584},
  year={2020},
  organization={PMLR}
}

@article{grathwohl2019your,
  title={Your classifier is secretly an energy based model and you should treat it like one},
  author={Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, J{\"o}rn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
  journal={arXiv preprint arXiv:1912.03263},
  year={2019}
}

@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011}
}

@article{lecun2006tutorial,
  title={A tutorial on energy-based learning},
  author={LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, M and Huang, F},
  journal={Predicting structured data},
  volume={1},
  number={0},
  year={2006}
}

@inproceedings{razavi2019generating,
  title={Generating diverse high-fidelity images with vq-vae-2},
  author={Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14866--14876},
  year={2019}
}

@article{vahdat2020nvae,
  title={NVAE: A Deep Hierarchical Variational Autoencoder},
  author={Vahdat, Arash and Kautz, Jan},
  journal={arXiv preprint arXiv:2007.03898},
  year={2020}
}

@inproceedings{kingma2018glow,
  title={Glow: Generative flow with invertible 1x1 convolutions},
  author={Kingma, Durk P and Dhariwal, Prafulla},
  booktitle={Advances in neural information processing systems},
  pages={10215--10224},
  year={2018}
}

@article{kingma2013auto-encoding,
  title = {Auto-Encoding Variational Bayes},
  journal = {International Conference on Learning Representations},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
}

@article{song2021train,
  title={How to Train Your Energy-Based Models},
  author={Song, Yang and Kingma, Diederik P},
  journal={arXiv preprint arXiv:2101.03288},
  year={2021}
}

@incollection{kingma2014semisupervised,
  title = {Semi-Supervised {{Learning}} with {{Deep Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Kingma, Diederik P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {3581--3589},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/janwillem/Cloud/Zotero/storage/4ZXJWPPA/Kingma et al. - 2014 - Semi-supervised Learning with Deep Generative Mode.pdf}
}

@inproceedings{chen2018isolating,
  title={Isolating sources of disentanglement in variational autoencoders},
  author={Chen, Ricky TQ and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
  booktitle={Advances in neural information processing systems},
  pages={2610--2620},
  year={2018}
}

@inproceedings{van2016conditional,
  title={Conditional image generation with pixelcnn decoders},
  author={Van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and Vinyals, Oriol and Graves, Alex and others},
  booktitle={Advances in neural information processing systems},
  pages={4790--4798},
  year={2016}
}

@inproceedings{welling2005exponential,
  title = {Exponential Family Harmoniums with an Application to Information Retrieval},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Welling, Max and {Rosen-zvi}, Michal and Hinton, Geoffrey E},
  editor = {Saul, L. and Weiss, Y. and Bottou, L.},
  year = {2004},
  volume = {17},
  pages = {1481--1488},
  publisher = {{MIT Press}}
}

@article{hinton2006fast,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  month = may,
  volume = {18},
  pages = {1527--1554},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco.2006.18.7.1527},
  abstract = {We show how to use ``complementary priors'' to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  file = {/Users/janwillem/Cloud/Zotero/storage/UA8ZDI3H/Hinton et al. - 2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf;/Users/janwillem/Cloud/Zotero/storage/CYWEPDE9/neco.2006.18.7.html},
  journal = {Neural Computation},
  number = {7}
}

@techreport{smolensky1986information,
  title = {Information Processing in Dynamical Systems: {{Foundations}} of Harmony Theory},
  shorttitle = {Information Processing in Dynamical Systems},
  author = {Smolensky, Paul},
  year = {1986},
  institution = {{Colorado Univ at Boulder Dept of Computer Science}}
}

@inproceedings{rezende2014stochastic,
  address = {{Bejing, China}},
  series = {Proceedings of {{Machine Learning Research}}},
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  volume = {32},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \textendash{} rules for gradient backpropagation through stochastic variables \textendash{} and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  publisher = {{PMLR}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  editor = {Xing, Eric P. and Jebara, Tony},
  month = jun,
  year = {2014},
  pages = {1278-1286},
  file = {/Users/janwillem/Zotero/storage/FDD9NQIT/Rezende - 2014 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf},
}

@inproceedings{che_your_2020,
	title = {Your {GAN} is {Secretly} an {Energy}-based {Model} and {You} {Should} {Use} {Discriminator} {Driven} {Latent} {Sampling}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/90525e70b7842930586545c6f1c9310c-Abstract.html},
	language = {en},
	urldate = {2021-01-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Che, Tong and Zhang, Ruixiang and Sohl-Dickstein, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\babak\\Zotero\\storage\\ZZFKXPGH\\Che et al. - 2020 - Your GAN is Secretly an Energy-based Model and You.pdf:application/pdf;Snapshot:C\:\\Users\\babak\\Zotero\\storage\\2Y596L9W\\90525e70b7842930586545c6f1c9310c-Abstract.html:text/html},
}

@article{karras2017progressive,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}

@article{wu2019amortized,
  title={Amortized Population Gibbs Samplers with Neural Sufficient Statistics},
  author={Wu, Hao and Zimmermann, Heiko and Sennesh, Eli and Le, Tuan Anh and van de Meent, Jan-Willem},
  journal={arXiv preprint arXiv:1911.01382},
  year={2019}
}

@inproceedings{hou2017deep,
  title={Deep feature consistent variational autoencoder},
  author={Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1133--1141},
  year={2017},
  organization={IEEE}
}


@inproceedings{locatello2019challenging,
  title={Challenging common assumptions in the unsupervised learning of disentangled representations},
  author={Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle={international conference on machine learning},
  pages={4114--4124},
  year={2019}
}

@inproceedings{kulkarni2015deep,
  title={Deep convolutional inverse graphics network},
  author={Kulkarni, Tejas D and Whitney, William F and Kohli, Pushmeet and Tenenbaum, Josh},
  booktitle={Advances in neural information processing systems},
  pages={2539--2547},
  year={2015}
}

@inproceedings{chen2016infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2172--2180},
  year={2016}
}

@article{higgins2016beta,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year={2016}
}

@article{younes1989parametric,
  title={Parametric inference for imperfectly observed Gibbsian fields},
  author={Younes, Laurent},
  journal={Probability theory and related fields},
  volume={82},
  number={4},
  pages={625--645},
  year={1989},
  publisher={Springer}
}

@inproceedings{swersky2011autoencoders,
  title={On autoencoders and score matching for energy based models},
  author={Swersky, Kevin and Ranzato, Marc'Aurelio and Buchman, David and Marlin, Benjamin M and de Freitas, Nando},
  booktitle={ICML},
  year={2011}
}

@article{hinton02training, 
author = {Hinton, Geoffrey E.}, 
title = {Training Products of Experts by Minimizing Contrastive Divergence}, 
year = {2002}, 
issue_date = {August 2002}, 
publisher = {MIT Press}, 
address = {Cambridge, MA, USA},
volume = {14}, 
number = {8}, 
issn = {0899-7667}, 
journal = {Neural Comput.}, 
month = aug, 
pages = {1771â€“1800}, 
numpages = {30} 
}